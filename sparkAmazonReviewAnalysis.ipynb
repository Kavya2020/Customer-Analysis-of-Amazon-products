{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUSTOMER REVIEW ANALYSIS OF AMAZON PRODUCTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to build a classifier that would understand the essence of a piece of review text and assign it the most appropriate classification i.e., positive review or negative review through nlp.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different text mining pre processing concepts were explored during this project. As the dataset is unbalanced we perfomed downsampling to randomly filter out some of the majority cases and trained dataset with different classifers and got better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /bin/python\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "from handyspark import *\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as fnc\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the hive table from the local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc =SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"delimiter\", \"\\t\").option(\"header\", \"true\").load(\"amazon_customer_reviews_req_col.tsv\")\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewcount_for_product =df.groupBy('name').agg(count(\"reviews_text\"))\n",
    "reviewcount_for_product.show()\n",
    "\n",
    "ratingCount = df.groupBy(\"reviews_rating\").count()\n",
    "ratingCount.show()\n",
    "\n",
    "df_product_rating = df.select(df.name, df.reviews_rating.cast(\"int\"))\n",
    "\n",
    "avgRatingForProd = df_product_rating.groupBy(\"name\").mean(\"reviews_rating\") \n",
    "avgRatingForProd.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to differentiate 'Positive' and 'Negative' reviews, we first converted our rating column by assigning\n",
    " positive sentiment as '1' for ratings 4 and 5,\n",
    " negative sentiment as '0' for ratings 1,2,3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution looks skewed as 'Positive' ratings are much more than 'Negative' ratings as we can see the volume of 'Negative' cases is very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon = df.selectExpr(\"cast(reviews_rating as int) as label\", \"reviews_text\")\n",
    "amazon = amazon.withColumn(\"label\",fnc.when(amazon[\"label\"]>=4,1).otherwise(0))\n",
    "amazon.show()\n",
    "\n",
    "ratingCount = amazon.groupBy(\"label\").count()\n",
    "ratingCount.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer breaks the review text into words. The output of the tokenizer is the tokenized words from the table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are most commonly used words which are not helpful in distinguishing a review from another is removed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD-IDF is a feature vectorization method used in text mining to reflect the importance of a team to a document in the corpus. Term Frequency(TF) is the number of times that word appears in the review while document frequency(df) is the number of reviews that contains the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashing TF and IDF features from pyspark is used to generate the term frequency vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the review. \n",
    "tokenizer = Tokenizer(inputCol=\"reviews_text\", outputCol=\"tokenized_words\")\n",
    "tokenizedWordsDF = tokenizer.transform(amazon)\n",
    "tokenizedWordsDF.show()\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"tokenized_words\", outputCol=\"filtered_words\")\n",
    "filteredWordsDF = remover.transform(tokenizedWordsDF)\n",
    "filteredWordsDF.show()\n",
    "\n",
    "# Convert to TF words vector\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"TF\")\n",
    "tfwordsDF = hashingTF.transform(filteredWordsDF)\n",
    "tfwordsDF.show()\n",
    "\n",
    "# Convert to IDF words vector, ensure to name the features as 'features'\n",
    "idf = IDF(inputCol=\"TF\", outputCol=\"features\")\n",
    "idfModel = idf.fit(tfwordsDF)\n",
    "idfwordsDF = idfModel.transform(tfwordsDF)\n",
    "idfwordsDF.show()\n",
    "\n",
    "for features_label in idfwordsDF.select(\"features\", \"label\").take(3):\n",
    "    print(features_label)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelined the different stages tokenizing, removing stopwords and td-idf vectorization by considering pipeline as an estimator to cross validator and BinaryClassificationEvaluator evaluated the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing set \n",
    "(train, test) = amazon.randomSplit([0.7, 0.3])\n",
    "\n",
    "# logistic regression instance\n",
    "lr = LogisticRegression(maxIter=5)\n",
    "\n",
    "# Use a pipeline to chain all transformers and estimators\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idfModel, lr])\n",
    "\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# ParamGridBuilder to construct a grid of parameters to search over.\n",
    "paramGrid = ParamGridBuilder().addGrid(hashingTF.numFeatures, [10, 50]).addGrid(lr.regParam, [0.1, 0.01]).build()\n",
    "\n",
    "# A CrossValidator with Estimator as pipeline, an evaluator BinaryClassificationEvaluator\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=4) \n",
    "\n",
    "# Cross-validation, to choose the best set of parameters.\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# Make predictions on test reviews. cvModel uses the best LR model with best parameters.\n",
    "predictions = cvModel.transform(test)\n",
    "# selected = predictions.select(\"reviews_text\", \"label\", \"probability\", \"prediction\").take(20)\n",
    "\n",
    "# Evaluate result with ROC\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "rocScore = evaluator.evaluate(predictions)\n",
    "print(\"Area under ROC score: \" + str(rocScore))\n",
    "\n",
    "# Plot ROC curve from the predicted results\n",
    "results = predictions.select(['probability', 'label'])\n",
    "results_collect = results.collect()\n",
    "results_list = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect]\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    " \n",
    "y_test = [i[1] for i in results_list]\n",
    "y_score = [i[0] for i in results_list]\n",
    " \n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Review Prediction')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see from the model result, the output is more biased to the positive reviews and 10% negative reviews so to balance the dataset we performed downsampling to randomly filter out some of the majority cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.toHandy().cols[['reviews_text','probability', 'prediction', 'label']][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOWN SAMPLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assigned a random integer to each positive class object, and then filtered out those whose objects with \"random integer\" is larger than a threshold which we calculated, so that the data points from the majority class -- 'Positive'-- will be mush less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble of Down Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time when we do a down sampling on training data, we are filtering out some data that belong to the \"Positive\" class, by doing this we will miss out information which could be used to train our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we tried down sampling for different data set to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Down Sampling\n",
    "\n",
    "ratio = 2.0 ## ratio of positive to negative for down sampling\n",
    "\n",
    "train_sample = train\n",
    "counts = train_sample.select('label').groupBy('label').count().collect()\n",
    "print(counts)\n",
    "higherBound = counts[0][1]\n",
    "thresholdToFilter = int(ratio * float(counts[1][1]) / counts[0][1] * higherBound)\n",
    " \n",
    "print(\"Higherbound: \", higherBound)\n",
    "print(\"Threshold to filter the majority class\", thresholdToFilter)\n",
    "\n",
    "train_sample = train_sample.withColumn(\"randIndex\",\n",
    "    fnc.when(train[\"label\"] == 1, round(rand()*(higherBound-1)+1,0)).\n",
    "    otherwise(-1))\n",
    "\n",
    "train1=train_sample.where(\"randIndex < 3920\")\n",
    "train2=train_sample.where(\"(randIndex > 3920 and randIndex < 7000) OR randIndex == -1\")\n",
    "train3=train_sample.where(\"(randIndex > 7000 and randIndex < 11000) OR randIndex == -1\")\n",
    "train4=train_sample.where(\"(randIndex > 11000 and randIndex < 17000) OR randIndex == -1\")\n",
    "\n",
    "print(train1.select('label').groupBy('label').count().collect())\n",
    "print(train2.select('label').groupBy('label').count().collect())\n",
    "print(train3.select('label').groupBy('label').count().collect())\n",
    "print(train4.select('label').groupBy('label').count().collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomforest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained downsample dataset with Randomforest classifier took the average from all the models trained with different downsampled data-sets we got better overall predictions with ROC score of 90%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [train1,train2,train3,train4]\n",
    "\n",
    "for i in range(0,len(df_list)):\n",
    "    train_df = df_list[i]\n",
    "    rf = RandomForestClassifier(numTrees=15)\n",
    "    pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idfModel, rf])\n",
    "\n",
    "    paramGrid = ParamGridBuilder().addGrid(hashingTF.numFeatures, [10, 50]).addGrid(rf.maxDepth, [5, 15]).build()\n",
    "\n",
    "    crossval = CrossValidator(estimator = pipeline,\n",
    "                          estimatorParamMaps = paramGrid,\n",
    "                          evaluator = BinaryClassificationEvaluator(),\n",
    "                          numFolds = 5)\n",
    "\n",
    "    cvModel = crossval.fit(train_df)\n",
    "    prediction = cvModel.transform(test)\n",
    "       \n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "    areaUnderROC = evaluator.evaluate(prediction)\n",
    "    print(\"area Under ROC score: \" + str(areaUnderROC))\n",
    "    \n",
    "    prediction.toHandy().cols[['probability', 'prediction', 'label']][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train1\n",
    "rf = RandomForestClassifier(numTrees=15)\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idfModel, rf])\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(hashingTF.numFeatures, [10, 50]).addGrid(rf.maxDepth, [5, 15]).build()\n",
    "\n",
    "crossval = CrossValidator(estimator = pipeline,\n",
    "                          estimatorParamMaps = paramGrid,\n",
    "                          evaluator = BinaryClassificationEvaluator(),\n",
    "                          numFolds = 5)\n",
    "\n",
    "cvModel = crossval.fit(train_df)\n",
    "prediction = cvModel.transform(test)\n",
    "\n",
    "results = prediction.select(['probability', 'label'])\n",
    "results_collect = results.collect()\n",
    "results_list = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect]\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    " \n",
    "y_test = [i[1] for i in results_list]\n",
    "y_score = [i[0] for i in results_list]\n",
    " \n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Review Prediction')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "    \n",
    "prediction.toHandy().cols[['probability', 'prediction', 'label']][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a GBT model.\n",
    "df_list = [train1,train2,train3,train4]\n",
    "\n",
    "for i in range(0,len(df_list)):\n",
    "    gbt = GBTClassifier(maxIter=10)\n",
    "\n",
    "    pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idfModel, gbt])\n",
    "\n",
    "    paramGrid = ParamGridBuilder().addGrid(hashingTF.numFeatures, [10, 50]).build()\n",
    "\n",
    "    crossval = CrossValidator(estimator = pipeline,\n",
    "                          estimatorParamMaps = paramGrid,\n",
    "                          evaluator = BinaryClassificationEvaluator(),\n",
    "                          numFolds = 3)\n",
    "\n",
    "    cvModel = crossval.fit(train1)\n",
    "\n",
    "    # Make predictions.\n",
    "    predictions = cvModel.transform(test)\n",
    "    # Select example rows to display.\n",
    "    selected_GBT = predictions.select(\"reviews_text\", \"label\", \"probability\", \"prediction\").take(5)\n",
    "    for row in selected_GBT:\n",
    "        print(row)\n",
    "\n",
    "    evaluator_GBT = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "    rocScore = evaluator_GBT.evaluate(predictions)\n",
    "    print(\"ROC score for Gradient-boosted tree classifier: \" + rocScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(maxIter=10)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idfModel, gbt])\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(hashingTF.numFeatures, [10, 50]).build()\n",
    "\n",
    "crossval = CrossValidator(estimator = pipeline,\n",
    "                          estimatorParamMaps = paramGrid,\n",
    "                          evaluator = BinaryClassificationEvaluator(),\n",
    "                          numFolds = 3)\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "cvModel = crossval.fit(train2)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = cvModel.transform(test)\n",
    "\n",
    "evaluator_GBT = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "Accuray_GBT = evaluator_GBT.evaluate(predictions)\n",
    "print(\"Accuracy for Gradient-boosted tree classifier: \" + str(Accuray_GBT))\n",
    "\n",
    "\n",
    "results = predictions.select(['probability', 'label'])\n",
    "results_collect = results.collect()\n",
    "results_list = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect]\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    " \n",
    "y_test = [i[1] for i in results_list]\n",
    "y_score = [i[0] for i in results_list]\n",
    " \n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Review Prediction')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "    \n",
    "predictions.toHandy().cols[['probability', 'prediction', 'label']][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Different NLP pre processing techniques and concepts were explored during this project from our analysis actually pre-processing steps are very important.Handling of misspelled words, incorrectly spelled words will be taken as they are during the training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
